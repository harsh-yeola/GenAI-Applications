{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"/dbfs/FileStore/risk/bank_risk_definitions.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf069ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks Vector Search uses Delta + embedding index.\n",
    "# Make sure your workspace has Databricks Vector Search enabled.\n",
    "# Create a Delta Table for storing embeddings\n",
    "table_name = \"risk_definitions_vectors\"\n",
    "\n",
    "# Create empty table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "  id STRING,\n",
    "  text STRING,\n",
    "  embedding ARRAY<FLOAT>\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22499c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings & write to Delta\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import uuid\n",
    "\n",
    "embed = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "data = []\n",
    "for chunk in chunks:\n",
    "    vector = embed.embed_query(chunk.page_content)\n",
    "    data.append((str(uuid.uuid4()), chunk.page_content, vector))\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"text\", \"embedding\"])\n",
    "df.write.mode(\"append\").format(\"delta\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Databricks Vector Index\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "vs = VectorSearchClient()\n",
    "\n",
    "endpoint_name = \"risk-endpoint\"\n",
    "index_name = \"risk_index\"\n",
    "\n",
    "vs.create_endpoint(name=endpoint_name)  # if not already created\n",
    "\n",
    "vs.create_delta_vector_search_index(\n",
    "    endpoint_name=endpoint_name,\n",
    "    index_name=index_name,\n",
    "    primary_key=\"id\",\n",
    "    table_name=table_name,\n",
    "    embedding_vector_column=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d63a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Retriever Wrapper\n",
    "from databricks.vector_search.client import VectorSearchIndex\n",
    "\n",
    "index = vs.get_index(endpoint_name, index_name)\n",
    "\n",
    "def retrieve_risk_context(query):\n",
    "    vector = embed.embed_query(query)\n",
    "\n",
    "    results = index.query(\n",
    "        query_vector=vector,\n",
    "        columns=[\"text\"],\n",
    "        k=4\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join([row[\"text\"] for row in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d84da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RunnableSequence LLM Classification Chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "classification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a senior banking risk officer.\n",
    "\n",
    "Bank-specific risk definitions:\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "\n",
    "Risk event:\n",
    "{issue_description}\n",
    "\n",
    "TASK:\n",
    "1. Tag the event with one or more bank-defined risk types. (Multi-label)\n",
    "2. Provide a 1â€“2 line summary.\n",
    "3. Output JSON:\n",
    "{{\n",
    "  \"risk_type\": [string],\n",
    "  \"issue_summary\": string\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        context=lambda x: retrieve_risk_context(x[\"issue_description\"]),\n",
    "        issue_description=RunnablePassthrough()\n",
    "    )\n",
    "    | classification_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6837ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Risk Events File from DBFS\n",
    "df = pd.read_csv(\"/dbfs/FileStore/risk/banking_dummy_issues.csv\")\n",
    "df.head()\n",
    "# spark.read.csv(\"/dbfs/FileStore/risk/banking_dummy_issues.csv\", header=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4356a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Entire DataFrame Using abatch()\n",
    "import asyncio\n",
    "\n",
    "async def process_df(df):\n",
    "    inputs = [{\"issue_description\": text} for text in df[\"issue_description\"]]\n",
    "\n",
    "    outputs = await rag_chain.abatch(inputs)\n",
    "\n",
    "    summaries = []\n",
    "    risk_types = []\n",
    "\n",
    "    for out in outputs:\n",
    "        content = out.content.strip()\n",
    "        try:\n",
    "            obj = eval(content)\n",
    "            summaries.append(obj[\"issue_summary\"])\n",
    "            risk_types.append(\", \".join(obj[\"risk_type\"]))\n",
    "        except:\n",
    "            summaries.append(\"Summary unavailable.\")\n",
    "            risk_types.append(\"Other\")\n",
    "\n",
    "    df[\"issue_summary\"] = summaries\n",
    "    df[\"risk_type\"] = risk_types\n",
    "    return df\n",
    "\n",
    "result_df = asyncio.run(process_df(df))\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to DBFS as CSV\n",
    "result_df.to_csv(\"/dbfs/FileStore/risk/risk_events_enriched.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Delta Table\n",
    "spark_df = spark.createDataFrame(result_df)\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"classified_risk_events\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
